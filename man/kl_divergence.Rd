% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kl_divergence.R
\name{kl_divergence}
\alias{kl_divergence}
\title{Calculate Kullback-Leibler Divergence}
\usage{
kl_divergence(P, Q, epsilon = 1e-07)
}
\arguments{
\item{P}{A numeric vector representing the first probability distribution.
The sum of "P" should be 1, but the function will normalize it if
necessary.}

\item{Q}{A numeric vector representing the second probability distribution.
The sum of "Q" should be 1, but the function will normalize it if
necessary.}

\item{epsilon}{A small positive number (default = 1e-7) to be added to each
probability in P and Q to avoid zero probabilities. This helps to
prevent division by zero or log(0).}
}
\value{
float
}
\description{
This function computes the Kullback-Leibler (KL) divergence between two
probability distributions, with an optional small constant (epsilon)
added to avoid zero probabilities, which would otherwise cause division
by zero or undefined logarithms.
}
\examples{
P <- c(0.1, 0.4, 0.3, 0.2)
Q <- c(0.2, 0.3, 0.4, 0.1)

kl_divergence(P, Q)

}
